{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/w3pzlt_11hdcpw6gskd45b_80000gn/T/ipykernel_11688/1944755676.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyankashrestha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyankashrestha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priyankashrestha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/priyankashrestha/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook performs the bag of words approach to distinguish between female and male classes.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import tqdm\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "# tokenizing (with stemming and lemmatizing)\n",
    "def custom_tokenizer(text):\n",
    "    # tokenize text by replacing punctuation and numbers with spaces and lowercase all words\n",
    "    punc_list = string.punctuation + '0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(t)\n",
    "    words = word_tokenize(text)\n",
    "    # filter out short words\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    # stem words\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    # lemmatize words\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    words = [lemmer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: import files and separate male/female\n",
    "# separating male and female notes\n",
    "df = pd.read_csv('./processed_data/sections_processed_filtered.csv')\n",
    "df = df.head(10000) \n",
    "df_f = df[df['1'] == 'F']  # female notes\n",
    "df_m = df[df['1'] == 'M']  # male notes\n",
    "\n",
    "# converting section object to list\n",
    "section_list = df['0'].values.tolist()\n",
    "section_list = [str(item) for item in section_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stop words\n",
    "my_stop_words = ['the','and','to','of','was','with','a','on','in','for','name',\n",
    "                 'is','patient','s','he','at','as','or','one','she','his','her','am',\n",
    "                 'were','you','pt','pm','by','be','had','your','this','date',\n",
    "                 'from','there','an','that','p','are','have','has','h','but','o',\n",
    "                 'namepattern','which','every','also']\n",
    "standard_stop = stopwords.words(\"english\")\n",
    "total_stop_words = list(set(my_stop_words + standard_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs229/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cs229/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['everi', 'ha', 'hi', 'thi', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Serialization\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=3000, stop_words=my_stop_words)\n",
    "tfidf.fit(section_list)\n",
    "Y = tfidf.transform(section_list)\n",
    "tfidf_names = list(tfidf.get_feature_names_out())\n",
    "\n",
    "with open(\"tfidf_labels_16000.txt\", \"w\") as txt_file:\n",
    "    for line in tfidf_names:\n",
    "        txt_file.write(\" \".join(line) + \"\\n\")\n",
    "numpyArray = Y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serialize NumPy array into JSON and write into a file\n",
      "Done writing serialized NumPy array into file\n"
     ]
    }
   ],
   "source": [
    "# Serialization\n",
    "numpyData = {\"array\": numpyArray}\n",
    "encodedNumpyData2 = json.dumps(numpyData, cls=NumpyArrayEncoder)  # use dump() to write array into file\n",
    "print(\"serialize NumPy array into JSON and write into a file\")\n",
    "with open(\"numpyData_tfidf_16000.json\", \"w\") as write_file:\n",
    "    json.dump(numpyData, write_file, cls=NumpyArrayEncoder)\n",
    "print(\"Done writing serialized NumPy array into file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And make a dataframe out of it\n",
    "results = pd.DataFrame(Y.toarray(), columns=tfidf.get_feature_names_out())\n",
    "results.to_json(r'tfidf.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonfiltered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: import files and separate male/female\n",
    "# separating male and female notes\n",
    "df = pd.read_csv('./processed_data/sections.csv')\n",
    "df = df.head(10000) \n",
    "df_f = df[df['1'] == 'F']  # female notes\n",
    "df_m = df[df['1'] == 'M']  # male notes\n",
    "\n",
    "# converting section object to list\n",
    "section_list = df['0'].values.tolist()\n",
    "section_list = [str(item) for item in section_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs229/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cs229/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['everi', 'ha', 'hi', 'thi', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Serialization\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=3000, stop_words=my_stop_words)\n",
    "tfidf.fit(section_list)\n",
    "Y = tfidf.transform(section_list)\n",
    "tfidf_names = list(tfidf.get_feature_names_out())\n",
    "\n",
    "with open(\"tfidf_labels_16000_unfiltered.txt\", \"w\") as txt_file:\n",
    "    for line in tfidf_names:\n",
    "        txt_file.write(\" \".join(line) + \"\\n\")\n",
    "numpyArray = Y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serialize NumPy array into JSON and write into a file\n",
      "Done writing serialized NumPy array into file\n"
     ]
    }
   ],
   "source": [
    "# Serialization\n",
    "numpyData = {\"array\": numpyArray}\n",
    "encodedNumpyData2 = json.dumps(numpyData, cls=NumpyArrayEncoder)  # use dump() to write array into file\n",
    "print(\"serialize NumPy array into JSON and write into a file\")\n",
    "with open(\"numpyData_tfidf_16000_unfiltered.json\", \"w\") as write_file:\n",
    "    json.dump(numpyData, write_file, cls=NumpyArrayEncoder)\n",
    "print(\"Done writing serialized NumPy array into file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization\n",
    "numpyData = {\"array\": numpyArray}\n",
    "encodedNumpyData2 = json.dumps(numpyData, cls=NumpyArrayEncoder)  # use dump() to write array into file\n",
    "print(\"serialize NumPy array into JSON and write into a file\")\n",
    "with open(\"numpyData_tfidf_16000_unfiltered.json\", \"w\") as write_file:\n",
    "    json.dump(numpyData, write_file, cls=NumpyArrayEncoder)\n",
    "print(\"Done writing serialized NumPy array into file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('cs229')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "461c55e28eaa3804e04bffb63d0c180b4c1903324abf59bdb8949f7a5cd3fa10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
